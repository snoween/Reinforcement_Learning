{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridworld setup\n",
    "grid_size = 5\n",
    "n_states = grid_size * grid_size\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "gamma = 0.95\n",
    "\n",
    "# Special squares (by index)\n",
    "blue = 1 * grid_size + 1      # (1,1)\n",
    "green = 3 * grid_size + 1     # (3,1)\n",
    "red = 1 * grid_size + 3       # (1,3)\n",
    "yellow = 3 * grid_size + 3    # (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e356911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map (row, col) to state index\n",
    "def state_index(row, col):\n",
    "    return row * grid_size + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement transitions\n",
    "def next_state(state, action):\n",
    "    row, col = divmod(state, grid_size)\n",
    "    if action == 'up':\n",
    "        row2 = max(row - 1, 0)\n",
    "        col2 = col\n",
    "    elif action == 'down':\n",
    "        row2 = min(row + 1, grid_size - 1)\n",
    "        col2 = col\n",
    "    elif action == 'left':\n",
    "        row2 = row\n",
    "        col2 = max(col - 1, 0)\n",
    "    elif action == 'right':\n",
    "        row2 = row\n",
    "        col2 = min(col + 1, grid_size - 1)\n",
    "    return state_index(row2, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward and next_state logic\n",
    "def transition(s, a):\n",
    "    if s == blue:\n",
    "        return red, 5\n",
    "    if s == green:\n",
    "        return np.random.choice([red, yellow]), 2.5\n",
    "    s2 = next_state(s, a)\n",
    "    if s == s2:  # Hitting boundary\n",
    "        return s, -0.5\n",
    "    return s2, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eae515",
   "metadata": {},
   "source": [
    "Question 1: (1) solving the system of Bellman equations explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving system Bellman\n",
    "def solve_bellman_system():\n",
    "    A = np.zeros((n_states, n_states))\n",
    "    b = np.zeros(n_states)\n",
    "    for s in range(n_states):\n",
    "        for a in actions:\n",
    "            s2, r = transition(s, a)\n",
    "            A[s, s2] += 0.25 * gamma\n",
    "            b[s] += 0.25 * r\n",
    "        A[s, s] -= 1  # Move all terms to one side: A * V = b\n",
    "    V = np.linalg.solve(A, -b)\n",
    "    return V.reshape((grid_size, grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e381bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_exact = solve_bellman_system()\n",
    "print(\"\\nValue Function (Exact Solution):\\n\", V_exact.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d25673",
   "metadata": {},
   "source": [
    "Question 1: (2) iterative policy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of Iterative policy evaluation\n",
    "def iterative_policy_evaluation(threshold=1e-2):\n",
    "    V = np.zeros(n_states)\n",
    "    policy_prob = 0.25\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            v = 0\n",
    "            for a in actions:\n",
    "                s2, r = transition(s, a)\n",
    "                v += policy_prob * (r + gamma * V[s2])\n",
    "            new_V[s] = v\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        V = new_V\n",
    "        iteration += 1\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    print(f\"Converged after {iteration} iterations.\")\n",
    "    return V.reshape((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6647e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_iter = iterative_policy_evaluation()\n",
    "print(\"Value Function (Iterative):\\n\", V_iter.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b1d30",
   "metadata": {},
   "source": [
    "Question 2: Determine the optimal policy for the gridworld problem by \n",
    "\n",
    "(1) explicitly solving the Bellman optimality equation \n",
    "\n",
    "(2) using policy iteration with iterative policy evaluation \n",
    "\n",
    "(3) policy improvement with value iteration.\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4df4f",
   "metadata": {},
   "source": [
    "(1) explicitly solving the Bellman optimality equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid_size, gamma, theta=1e-6):\n",
    "    V = np.zeros((grid_size, grid_size))\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]\n",
    "                values = []\n",
    "                for idx, action in enumerate(actions):\n",
    "                    # Replace take_action with transition\n",
    "                    s = i * grid_size + j\n",
    "                    s2, r = transition(s, action)\n",
    "                    row2, col2 = divmod(s2, grid_size)\n",
    "                    values.append(r + gamma * V[row2, col2])\n",
    "                V[i, j] = max(values)\n",
    "                policy[i, j] = np.argmax(values)\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_opt, policy_opt = value_iteration(5, 0.95)\n",
    "print(np.round(V_opt, 3))\n",
    "print(policy_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5d8db",
   "metadata": {},
   "source": [
    "(2) Policy Iteration with Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7637932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(threshold=1e-2):\n",
    "    V = np.zeros(n_states)\n",
    "    policy = np.full((n_states,), 'up', dtype=object)  # arbitrary init\n",
    "    stable = False\n",
    "    iter_count = 0\n",
    "\n",
    "    while not stable:\n",
    "        # POLICY EVALUATION\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = np.copy(V)\n",
    "            for s in range(n_states):\n",
    "                a = policy[s]\n",
    "                s2, r = transition(s, a)\n",
    "                new_V[s] = r + gamma * V[s2]\n",
    "                delta = max(delta, abs(new_V[s] - V[s]))\n",
    "            V = new_V\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        # POLICY IMPROVEMENT\n",
    "        stable = True\n",
    "        for s in range(n_states):\n",
    "            old_a = policy[s]\n",
    "            values = []\n",
    "            for a in actions:\n",
    "                s2, r = transition(s, a)\n",
    "                values.append(r + gamma * V[s2])\n",
    "            best_a = actions[np.argmax(values)]\n",
    "            policy[s] = best_a\n",
    "            if best_a != old_a:\n",
    "                stable = False\n",
    "        iter_count += 1\n",
    "\n",
    "    print(f\"Policy Iteration converged after {iter_count} improvement steps.\")\n",
    "    return V.reshape((grid_size, grid_size)), policy.reshape((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba36b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_pi, policy_pi = policy_iteration()\n",
    "\n",
    "print(\"Optimal Value Function (Policy Iteration):\\n\", V_pi.round(2))\n",
    "print(\"Optimal Policy (Policy Iteration):\\n\", policy_pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0467049",
   "metadata": {},
   "source": [
    "(3) Policy Improvement with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(threshold=1e-4):\n",
    "    V = np.zeros(n_states)\n",
    "    iter_count = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for s in range(n_states):\n",
    "            values = []\n",
    "            for a in actions:\n",
    "                s2, r = transition(s, a)\n",
    "                values.append(r + gamma * V[s2])\n",
    "            new_V[s] = max(values)\n",
    "            delta = max(delta, abs(new_V[s] - V[s]))\n",
    "        V = new_V\n",
    "        iter_count += 1\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    # Derive policy\n",
    "    policy = np.full((n_states,), 'up', dtype=object)\n",
    "    for s in range(n_states):\n",
    "        values = []\n",
    "        for a in actions:\n",
    "            s2, r = transition(s, a)\n",
    "            values.append(r + gamma * V[s2])\n",
    "        policy[s] = actions[np.argmax(values)]\n",
    "\n",
    "    print(f\"Value Iteration converged after {iter_count} iterations.\")\n",
    "    return V.reshape((grid_size, grid_size)), policy.reshape((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487028d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_vi, policy_vi = value_iteration()\n",
    "\n",
    "print(\"\\nOptimal Value Function (Value Iteration):\\n\", V_vi.round(2))\n",
    "print(\"Optimal Policy (Value Iteration):\\n\", policy_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cfd0e",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4aef2a",
   "metadata": {},
   "source": [
    "Question 2: Use the Monte Carlo method with (1) exploring starts and (2) without exploring starts but the\n",
    "ε-soft approach to learn an optimal policy for this modified gridworld problem. Use the same\n",
    "discount factor of γ = 0.95 as you have in the Part 1 above. You can start with a policy with\n",
    "equiprobable moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6483cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLACK = terminal states, put it at top left and bottom right \n",
    "black_states = [0, 24]\n",
    "\n",
    "# Overwrite transition() to include terminal logic and new reward\n",
    "def transition_with_terminal(s, a):\n",
    "    if s in black_states:\n",
    "        return s, 0  # Terminal state\n",
    "    if s == blue:\n",
    "        return red, 5\n",
    "    if s == green:\n",
    "        return np.random.choice([red, yellow]), 2.5\n",
    "    s2 = next_state(s, a)\n",
    "    if s == s2:\n",
    "        return s, -0.5  # hitting wall\n",
    "    return s2, -0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330abfd5",
   "metadata": {},
   "source": [
    " (1) exploring starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_exploring_starts(num_episodes=1000, gamma=0.95):\n",
    "    Q = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    returns = { (s, a): [] for s in range(n_states) for a in actions }\n",
    "    policy = { s: np.random.choice(actions) for s in range(n_states) }\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Exploring Start: random (s,a)\n",
    "        s = np.random.choice([s for s in range(n_states) if s not in black_states])\n",
    "        a = np.random.choice(actions)\n",
    "\n",
    "        episode = []\n",
    "        while True:\n",
    "            s2, r = transition_with_terminal(s, a)\n",
    "            episode.append((s, a, r))\n",
    "            if s2 in black_states:\n",
    "                break\n",
    "            s = s2\n",
    "            a = policy[s]\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(returns[(s, a)])\n",
    "                visited.add((s, a))\n",
    "                # Improve policy greedily\n",
    "                best_a = max(actions, key=lambda a_: Q[(s, a_)])\n",
    "                policy[s] = best_a\n",
    "\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142dec1",
   "metadata": {},
   "source": [
    " (2) without exploring starts but the\n",
    "ε-soft approach to learn an optimal policy for this modified gridworld problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0adfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_soft(num_episodes=10000, gamma=0.95, epsilon=0.1):\n",
    "    Q = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    returns = { (s, a): [] for s in range(n_states) for a in actions }\n",
    "    policy = { s: { a: 1/len(actions) for a in actions } for s in range(n_states) }\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Start from random non-terminal state\n",
    "        s = np.random.choice([s for s in range(n_states) if s not in black_states])\n",
    "        episode = []\n",
    "\n",
    "        while True:\n",
    "            a = np.random.choice(actions, p=[policy[s][a_] for a_ in actions])\n",
    "            s2, r = transition_with_terminal(s, a)\n",
    "            episode.append((s, a, r))\n",
    "            if s2 in black_states:\n",
    "                break\n",
    "            s = s2\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(returns[(s, a)])\n",
    "                visited.add((s, a))\n",
    "                # Improve policy using epsilon-greedy\n",
    "                best_a = max(actions, key=lambda a_: Q[(s, a_)])\n",
    "                for a_ in actions:\n",
    "                    if a_ == best_a:\n",
    "                        policy[s][a_] = 1 - epsilon + (epsilon / len(actions))\n",
    "                    else:\n",
    "                        policy[s][a_] = epsilon / len(actions)\n",
    "\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1fcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result in a matrix form\n",
    "def format_policy(policy_dict):\n",
    "    table = np.full((grid_size, grid_size), '', dtype=object)\n",
    "    for s in range(n_states):\n",
    "        r, c = divmod(s, grid_size)\n",
    "        if isinstance(policy_dict[s], str):\n",
    "            table[r, c] = policy_dict[s]\n",
    "        else:\n",
    "            best_a = max(policy_dict[s], key=policy_dict[s].get)\n",
    "            table[r, c] = best_a\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbec4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Starts\n",
    "Q_es, policy_es = mc_control_exploring_starts()\n",
    "policy_grid_es = format_policy(policy_es)\n",
    "print(\"Policy (Exploring Starts):\\n\", policy_grid_es)\n",
    "\n",
    "# Epsilon-Soft\n",
    "Q_eps, policy_eps = mc_control_epsilon_soft()\n",
    "policy_grid_eps = format_policy(policy_eps)\n",
    "print(\"\\nPolicy (Epsilon-Soft):\\n\", policy_grid_eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb04f6",
   "metadata": {},
   "source": [
    "Question 2: Now use a behaviour policy with equiprobable moves to learn an optimal policy. Note here the\n",
    "dynamics of the world are known exactly, so you can actually compute the importance weights\n",
    "needed for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_offpolicy_importance_sampling(num_episodes=10000, gamma=0.95):\n",
    "    Q = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    C = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    target_policy = { s: np.random.choice(actions) for s in range(n_states) }\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Generate episode using behaviour policy (random)\n",
    "        s = np.random.choice([s for s in range(n_states) if s not in black_states])\n",
    "        episode = []\n",
    "\n",
    "        while True:\n",
    "            a = np.random.choice(actions)  # behaviour = random\n",
    "            s2, r = transition_with_terminal(s, a)\n",
    "            episode.append((s, a, r))\n",
    "            if s2 in black_states:\n",
    "                break\n",
    "            s = s2\n",
    "\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "\n",
    "            # Improve target policy\n",
    "            best_a = max(actions, key=lambda a_: Q[(s, a_)])\n",
    "            target_policy[s] = best_a\n",
    "\n",
    "            if a != target_policy[s]:\n",
    "                break  # importance weight becomes 0 from here on\n",
    "            W = W * (1.0 / 0.25)  # π(a|s)=1, b(a|s)=0.25\n",
    "\n",
    "    return Q, target_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473426ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_off, policy_off = mc_offpolicy_importance_sampling()\n",
    "policy_grid_off = format_policy(policy_off)\n",
    "\n",
    "print(\"Optimal Policy (Off-policy MC with Importance Sampling):\\n\", policy_grid_off)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_COMP6915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
