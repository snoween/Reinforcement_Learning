{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95be9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1f55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridworld setup\n",
    "grid_size = 5\n",
    "n_states = grid_size * grid_size\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "gamma = 0.95\n",
    "\n",
    "# Special squares (by index)\n",
    "blue = 1 * grid_size + 1      # (1,1)\n",
    "green = 3 * grid_size + 1     # (3,1)\n",
    "red = 1 * grid_size + 3       # (1,3)\n",
    "yellow = 3 * grid_size + 3    # (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e356911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map (row, col) to state index\n",
    "def state_index(row, col):\n",
    "    return row * grid_size + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5d8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement transitions\n",
    "def next_state(state, action):\n",
    "    row, col = divmod(state, grid_size)\n",
    "    if action == 'up':\n",
    "        row2 = max(row - 1, 0)\n",
    "        col2 = col\n",
    "    elif action == 'down':\n",
    "        row2 = min(row + 1, grid_size - 1)\n",
    "        col2 = col\n",
    "    elif action == 'left':\n",
    "        row2 = row\n",
    "        col2 = max(col - 1, 0)\n",
    "    elif action == 'right':\n",
    "        row2 = row\n",
    "        col2 = min(col + 1, grid_size - 1)\n",
    "    return state_index(row2, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a50def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward and next_state logic\n",
    "def transition(s, a):\n",
    "    if s == blue:\n",
    "        return red, 5\n",
    "    if s == green:\n",
    "        return np.random.choice([red, yellow]), 2.5\n",
    "    s2 = next_state(s, a)\n",
    "    if s == s2:  # Hitting boundary\n",
    "        return s, -0.5\n",
    "    return s2, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eae515",
   "metadata": {},
   "source": [
    "Question 1: (1) solving the system of Bellman equations explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving system Bellman\n",
    "def solve_bellman_system():\n",
    "    A = np.zeros((n_states, n_states))\n",
    "    b = np.zeros(n_states)\n",
    "    for s in range(n_states):\n",
    "        for a in actions:\n",
    "            s2, r = transition(s, a)\n",
    "            A[s, s2] += 0.25 * gamma\n",
    "            b[s] += 0.25 * r\n",
    "        A[s, s] -= 1  # Move all terms to one side: A * V = b\n",
    "    V = np.linalg.solve(A, -b)\n",
    "    return V.reshape((grid_size, grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e381bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Function (Exact Solution):\n",
      " [[ 2.28  2.94  1.74  0.66 -0.11]\n",
      " [ 3.15  5.95  2.53  1.    0.15]\n",
      " [ 2.42  3.22  1.94  0.87  0.13]\n",
      " [ 1.93  3.26  1.57  0.59 -0.08]\n",
      " [ 1.04  1.43  0.81  0.13 -0.45]]\n"
     ]
    }
   ],
   "source": [
    "V_exact = solve_bellman_system()\n",
    "print(\"\\nValue Function (Exact Solution):\\n\", V_exact.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d25673",
   "metadata": {},
   "source": [
    "Question 1: (2) iterative policy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310d34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of Iterative policy evaluation\n",
    "def iterative_policy_evaluation(threshold=1e-2):\n",
    "    V = np.zeros(n_states)\n",
    "    policy_prob = 0.25\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            v = 0\n",
    "            for a in actions:\n",
    "                s2, r = transition(s, a)\n",
    "                v += policy_prob * (r + gamma * V[s2])\n",
    "            new_V[s] = v\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        V = new_V\n",
    "        iteration += 1\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    print(f\"Converged after {iteration} iterations.\")\n",
    "    return V.reshape((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6647e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 169 iterations.\n",
      "Value Function (Iterative):\n",
      " [[ 2.28  2.94  1.74  0.65 -0.11]\n",
      " [ 3.15  5.94  2.52  1.    0.15]\n",
      " [ 2.43  3.22  1.95  0.87  0.13]\n",
      " [ 1.93  3.25  1.57  0.59 -0.09]\n",
      " [ 1.05  1.43  0.81  0.13 -0.46]]\n"
     ]
    }
   ],
   "source": [
    "V_iter = iterative_policy_evaluation()\n",
    "print(\"Value Function (Iterative):\\n\", V_iter.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b1d30",
   "metadata": {},
   "source": [
    "Question 2: Determine the optimal policy for the gridworld problem by \n",
    "\n",
    "(1) explicitly solving the Bellman optimality equation \n",
    "\n",
    "(2) using policy iteration with iterative policy evaluation \n",
    "\n",
    "(3) policy improvement with value iteration.\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4df4f",
   "metadata": {},
   "source": [
    "(1) explicitly solving the Bellman optimality equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d5b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid_size, gamma, theta=1e-6):\n",
    "    V = np.zeros((grid_size, grid_size))\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]\n",
    "                values = []\n",
    "                for idx, action in enumerate(actions):\n",
    "                    # Replace take_action with transition\n",
    "                    s = i * grid_size + j\n",
    "                    s2, r = transition(s, action)\n",
    "                    row2, col2 = divmod(s2, grid_size)\n",
    "                    values.append(r + gamma * V[row2, col2])\n",
    "                V[i, j] = max(values)\n",
    "                policy[i, j] = np.argmax(values)\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ca9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31.639 33.304 31.639 30.057 28.554]\n",
      " [33.304 35.057 33.304 31.639 30.057]\n",
      " [31.639 33.304 31.639 30.057 28.554]\n",
      " [30.929 32.557 30.929 29.383 27.914]\n",
      " [29.383 30.929 29.383 27.914 26.518]]\n",
      "[[1 1 1 1 1]\n",
      " [3 0 2 2 2]\n",
      " [0 0 0 0 0]\n",
      " [3 0 2 2 2]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "V_opt, policy_opt = value_iteration(5, 0.95)\n",
    "print(np.round(V_opt, 3))\n",
    "print(policy_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5d8db",
   "metadata": {},
   "source": [
    "(2) Policy Iteration with Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7637932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(threshold=1e-2):\n",
    "    V = np.zeros(n_states)\n",
    "    policy = np.full((n_states,), 'up', dtype=object)  # arbitrary init\n",
    "    stable = False\n",
    "    iter_count = 0\n",
    "\n",
    "    while not stable:\n",
    "        # POLICY EVALUATION\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_V = np.copy(V)\n",
    "            for s in range(n_states):\n",
    "                a = policy[s]\n",
    "                s2, r = transition(s, a)\n",
    "                new_V[s] = r + gamma * V[s2]\n",
    "                delta = max(delta, abs(new_V[s] - V[s]))\n",
    "            V = new_V\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        # POLICY IMPROVEMENT\n",
    "        stable = True\n",
    "        for s in range(n_states):\n",
    "            old_a = policy[s]\n",
    "            values = []\n",
    "            for a in actions:\n",
    "                s2, r = transition(s, a)\n",
    "                values.append(r + gamma * V[s2])\n",
    "            best_a = actions[np.argmax(values)]\n",
    "            policy[s] = best_a\n",
    "            if best_a != old_a:\n",
    "                stable = False\n",
    "        iter_count += 1\n",
    "\n",
    "    print(f\"Policy Iteration converged after {iter_count} improvement steps.\")\n",
    "    return V.reshape((grid_size, grid_size)), policy.reshape((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba36b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_pi, policy_pi = policy_iteration()\n",
    "\n",
    "print(\"Optimal Value Function (Policy Iteration):\\n\", V_pi.round(2))\n",
    "print(\"Optimal Policy (Policy Iteration):\\n\", policy_pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0467049",
   "metadata": {},
   "source": [
    "(3) Policy Improvement with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(threshold=1e-4):\n",
    "    V = np.zeros(n_states)\n",
    "    iter_count = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for s in range(n_states):\n",
    "            values = []\n",
    "            for a in actions:\n",
    "                s2, r = transition(s, a)\n",
    "                values.append(r + gamma * V[s2])\n",
    "            new_V[s] = max(values)\n",
    "            delta = max(delta, abs(new_V[s] - V[s]))\n",
    "        V = new_V\n",
    "        iter_count += 1\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    # Derive policy\n",
    "    policy = np.full((n_states,), 'up', dtype=object)\n",
    "    for s in range(n_states):\n",
    "        values = []\n",
    "        for a in actions:\n",
    "            s2, r = transition(s, a)\n",
    "            values.append(r + gamma * V[s2])\n",
    "        policy[s] = actions[np.argmax(values)]\n",
    "\n",
    "    print(f\"Value Iteration converged after {iter_count} iterations.\")\n",
    "    return V.reshape((grid_size, grid_size)), policy.reshape((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487028d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged after 213 iterations.\n",
      "\n",
      "Optimal Value Function (Value Iteration):\n",
      " [[31.64 33.3  31.64 30.06 28.55]\n",
      " [33.3  35.06 33.3  31.64 30.06]\n",
      " [31.64 33.3  31.64 30.06 28.55]\n",
      " [30.93 32.56 30.93 29.38 27.91]\n",
      " [29.38 30.93 29.38 27.91 26.52]]\n",
      "Optimal Policy (Value Iteration):\n",
      " [['down' 'down' 'down' 'down' 'down']\n",
      " ['right' 'up' 'left' 'left' 'left']\n",
      " ['up' 'up' 'up' 'up' 'up']\n",
      " ['right' 'down' 'left' 'left' 'left']\n",
      " ['up' 'up' 'up' 'up' 'up']]\n"
     ]
    }
   ],
   "source": [
    "V_vi, policy_vi = value_iteration()\n",
    "\n",
    "print(\"\\nOptimal Value Function (Value Iteration):\\n\", V_vi.round(2))\n",
    "print(\"Optimal Policy (Value Iteration):\\n\", policy_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cfd0e",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4aef2a",
   "metadata": {},
   "source": [
    "Question 2: Use the Monte Carlo method with (1) exploring starts and (2) without exploring starts but the\n",
    "ε-soft approach to learn an optimal policy for this modified gridworld problem. Use the same\n",
    "discount factor of γ = 0.95 as you have in the Part 1 above. You can start with a policy with\n",
    "equiprobable moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6483cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLACK = terminal states, put it at top left and bottom right \n",
    "black_states = [0, 24]\n",
    "\n",
    "# Overwrite transition() to include terminal logic and new reward\n",
    "def transition_with_terminal(s, a):\n",
    "    if s in black_states:\n",
    "        return s, 0  # Terminal state\n",
    "    if s == blue:\n",
    "        return red, 5\n",
    "    if s == green:\n",
    "        return np.random.choice([red, yellow]), 2.5\n",
    "    s2 = next_state(s, a)\n",
    "    if s == s2:\n",
    "        return s, -0.5  # hitting wall\n",
    "    return s2, -0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330abfd5",
   "metadata": {},
   "source": [
    " (1) exploring starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_exploring_starts(num_episodes=1000, gamma=0.95):\n",
    "    Q = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    returns = { (s, a): [] for s in range(n_states) for a in actions }\n",
    "    policy = { s: np.random.choice(actions) for s in range(n_states) }\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Exploring Start: random (s,a)\n",
    "        s = np.random.choice([s for s in range(n_states) if s not in black_states])\n",
    "        a = np.random.choice(actions)\n",
    "\n",
    "        episode = []\n",
    "        while True:\n",
    "            s2, r = transition_with_terminal(s, a)\n",
    "            episode.append((s, a, r))\n",
    "            if s2 in black_states:\n",
    "                break\n",
    "            s = s2\n",
    "            a = policy[s]\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(returns[(s, a)])\n",
    "                visited.add((s, a))\n",
    "                # Improve policy greedily\n",
    "                best_a = max(actions, key=lambda a_: Q[(s, a_)])\n",
    "                policy[s] = best_a\n",
    "\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142dec1",
   "metadata": {},
   "source": [
    " (2) without exploring starts but the\n",
    "ε-soft approach to learn an optimal policy for this modified gridworld problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0adfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_soft(num_episodes=10000, gamma=0.95, epsilon=0.1):\n",
    "    Q = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    returns = { (s, a): [] for s in range(n_states) for a in actions }\n",
    "    policy = { s: { a: 1/len(actions) for a in actions } for s in range(n_states) }\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Start from random non-terminal state\n",
    "        s = np.random.choice([s for s in range(n_states) if s not in black_states])\n",
    "        episode = []\n",
    "\n",
    "        while True:\n",
    "            a = np.random.choice(actions, p=[policy[s][a_] for a_ in actions])\n",
    "            s2, r = transition_with_terminal(s, a)\n",
    "            episode.append((s, a, r))\n",
    "            if s2 in black_states:\n",
    "                break\n",
    "            s = s2\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(returns[(s, a)])\n",
    "                visited.add((s, a))\n",
    "                # Improve policy using epsilon-greedy\n",
    "                best_a = max(actions, key=lambda a_: Q[(s, a_)])\n",
    "                for a_ in actions:\n",
    "                    if a_ == best_a:\n",
    "                        policy[s][a_] = 1 - epsilon + (epsilon / len(actions))\n",
    "                    else:\n",
    "                        policy[s][a_] = epsilon / len(actions)\n",
    "\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1fcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result in a matrix form\n",
    "def format_policy(policy_dict):\n",
    "    table = np.full((grid_size, grid_size), '', dtype=object)\n",
    "    for s in range(n_states):\n",
    "        r, c = divmod(s, grid_size)\n",
    "        if isinstance(policy_dict[s], str):\n",
    "            table[r, c] = policy_dict[s]\n",
    "        else:\n",
    "            best_a = max(policy_dict[s], key=policy_dict[s].get)\n",
    "            table[r, c] = best_a\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbec4bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Exploring Starts\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Q_es, policy_es \u001b[38;5;241m=\u001b[39m \u001b[43mmc_control_exploring_starts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m policy_grid_es \u001b[38;5;241m=\u001b[39m format_policy(policy_es)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy (Exploring Starts):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, policy_grid_es)\n",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m, in \u001b[0;36mmc_control_exploring_starts\u001b[1;34m(num_episodes, gamma)\u001b[0m\n\u001b[0;32m     11\u001b[0m episode \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     s2, r \u001b[38;5;241m=\u001b[39m \u001b[43mtransition_with_terminal\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     episode\u001b[38;5;241m.\u001b[39mappend((s, a, r))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s2 \u001b[38;5;129;01min\u001b[39;00m black_states:\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mtransition_with_terminal\u001b[1;34m(s, a)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m==\u001b[39m green:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([red, yellow]), \u001b[38;5;241m2.5\u001b[39m\n\u001b[1;32m---> 12\u001b[0m s2 \u001b[38;5;241m=\u001b[39m \u001b[43mnext_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m==\u001b[39m s2:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# hitting wall\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mnext_state\u001b[1;34m(state, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     row2 \u001b[38;5;241m=\u001b[39m row\n\u001b[0;32m     15\u001b[0m     col2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(col \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, grid_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mstate_index\u001b[1;34m(row, col)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Map (row, col) to state index\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstate_index\u001b[39m(row, col):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row \u001b[38;5;241m*\u001b[39m grid_size \u001b[38;5;241m+\u001b[39m col\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Exploring Starts\n",
    "Q_es, policy_es = mc_control_exploring_starts()\n",
    "policy_grid_es = format_policy(policy_es)\n",
    "print(\"Policy (Exploring Starts):\\n\", policy_grid_es)\n",
    "\n",
    "# Epsilon-Soft\n",
    "Q_eps, policy_eps = mc_control_epsilon_soft()\n",
    "policy_grid_eps = format_policy(policy_eps)\n",
    "print(\"\\nPolicy (Epsilon-Soft):\\n\", policy_grid_eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb04f6",
   "metadata": {},
   "source": [
    "Question 2: Now use a behaviour policy with equiprobable moves to learn an optimal policy. Note here the\n",
    "dynamics of the world are known exactly, so you can actually compute the importance weights\n",
    "needed for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_offpolicy_importance_sampling(num_episodes=10000, gamma=0.95):\n",
    "    Q = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    C = { (s, a): 0.0 for s in range(n_states) for a in actions }\n",
    "    target_policy = { s: np.random.choice(actions) for s in range(n_states) }\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Generate episode using behaviour policy (random)\n",
    "        s = np.random.choice([s for s in range(n_states) if s not in black_states])\n",
    "        episode = []\n",
    "\n",
    "        while True:\n",
    "            a = np.random.choice(actions)  # behaviour = random\n",
    "            s2, r = transition_with_terminal(s, a)\n",
    "            episode.append((s, a, r))\n",
    "            if s2 in black_states:\n",
    "                break\n",
    "            s = s2\n",
    "\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "\n",
    "            # Improve target policy\n",
    "            best_a = max(actions, key=lambda a_: Q[(s, a_)])\n",
    "            target_policy[s] = best_a\n",
    "\n",
    "            if a != target_policy[s]:\n",
    "                break  # importance weight becomes 0 from here on\n",
    "            W = W * (1.0 / 0.25)  # π(a|s)=1, b(a|s)=0.25\n",
    "\n",
    "    return Q, target_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473426ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_off, policy_off = mc_offpolicy_importance_sampling()\n",
    "policy_grid_off = format_policy(policy_off)\n",
    "\n",
    "print(\"Optimal Policy (Off-policy MC with Importance Sampling):\\n\", policy_grid_off)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_COMP6915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
